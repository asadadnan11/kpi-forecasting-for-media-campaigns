{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Media Campaign KPI Forecasting\n",
    "\n",
    "Working on this project for my portfolio after finishing my MSBA program. Want to see if I can apply time series forecasting to predict campaign performance and optimize budget allocation.\n",
    "\n",
    "Been reading about ARIMA and ETS models for forecasting - seems like they could work well for marketing data that has trends and seasonality patterns.\n",
    "\n",
    "Plan:\n",
    "- Generate some realistic campaign data (don't have access to real data yet)\n",
    "- Explore the data to understand patterns\n",
    "- Build forecasting models using statsmodels \n",
    "- Test different budget scenarios based on predictions\n",
    "- See what actionable insights I can get\n",
    "\n",
    "Should be interesting to see if these statistical approaches can actually help with marketing decisions.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Setting up the libraries I'll need. Going with pandas/numpy for data manipulation and statsmodels for the time series forecasting part - heard it's the standard for this kind of analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # keeps output cleaner\n",
    "\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# might try plotly later for interactive charts\n",
    "\n",
    "# time series forecasting libraries\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.exponential_smoothing.ets import ETSModel\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller  # for checking stationarity\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "\n",
    "# set some display options to keep things organized\n",
    "pd.set_option('display.max_columns', 12)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"libraries loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "Since I don't have access to real campaign data (yet), I'll create synthetic data that mimics realistic campaign patterns. Want to include:\n",
    "- weekly seasonality (weekends typically perform differently)\n",
    "- gradual trends over time (campaign optimization effects)\n",
    "- logical relationships between metrics (impressions → clicks → conversions)\n",
    "- random variation to simulate real-world noise\n",
    "\n",
    "Going with 180 days of data - should be enough for proper training and testing of the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_campaign_data(days=180, start_date='2024-01-01'):\n",
    "    \"\"\"\n",
    "    Create synthetic campaign data with realistic patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)  # for reproducible results\n",
    "    \n",
    "    dates = pd.date_range(start=start_date, periods=days, freq='D')\n",
    "    \n",
    "    # base spend with linear trend (gradual budget increase)\n",
    "    base_spend = np.linspace(1000, 1200, days)  # campaign ramp-up\n",
    "    \n",
    "    # weekly seasonality using sine wave\n",
    "    weekly_pattern = np.sin(2 * np.pi * np.arange(days) / 7) * 0.15 + 1\n",
    "    \n",
    "    # add some longer-term cycles\n",
    "    monthly_pattern = np.sin(2 * np.pi * np.arange(days) / 30) * 0.1 + 1\n",
    "    \n",
    "    # random noise component\n",
    "    noise = np.random.normal(0, 0.08, days)\n",
    "    \n",
    "    # combine all components for daily spend\n",
    "    daily_spend = base_spend * weekly_pattern * monthly_pattern * (1 + noise)\n",
    "    daily_spend = np.maximum(daily_spend, 500)  # minimum $500/day\n",
    "    \n",
    "    # impressions based on spend and CPM\n",
    "    cpm_base = 2.5  # $2.50 CPM baseline\n",
    "    cpm_variation = np.random.normal(0, 0.3, days)  \n",
    "    impressions = (daily_spend / (cpm_base + cpm_variation)) * 1000\n",
    "    impressions = np.maximum(impressions, 10000)  \n",
    "    \n",
    "    # clicks with day-of-week effects (weekends typically worse)\n",
    "    base_ctr = 0.025  # 2.5% baseline CTR\n",
    "    ctr_improvement = np.linspace(0, 0.008, days)  # optimization over time\n",
    "    \n",
    "    # day of week multipliers based on typical performance\n",
    "    weekday_effects = np.array([0.8, 1.0, 1.1, 1.2, 1.15, 0.9, 0.7])  # Mon-Sun\n",
    "    day_multiplier = np.array([weekday_effects[date.weekday()] for date in dates])\n",
    "    \n",
    "    ctr = (base_ctr + ctr_improvement) * day_multiplier * (1 + np.random.normal(0, 0.1, days))\n",
    "    clicks = impressions * ctr\n",
    "    \n",
    "    # conversions with improvement over time\n",
    "    base_cvr = 0.08  # 8% conversion rate to start\n",
    "    cvr_improvement = np.linspace(0, 0.02, days)  # landing page optimization\n",
    "    cvr = (base_cvr + cvr_improvement) * (1 + np.random.normal(0, 0.15, days))\n",
    "    conversions = clicks * cvr\n",
    "    \n",
    "    # calculate cost metrics\n",
    "    cpc = daily_spend / clicks\n",
    "    cpa = daily_spend / conversions\n",
    "    \n",
    "    # revenue with variable AOV\n",
    "    avg_order_value = np.random.normal(150, 25, days)\n",
    "    avg_order_value = np.maximum(avg_order_value, 50)  \n",
    "    revenue = conversions * avg_order_value\n",
    "    \n",
    "    # return on ad spend\n",
    "    roas = revenue / daily_spend\n",
    "    \n",
    "    # create the final dataset\n",
    "    data = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'spend': daily_spend,\n",
    "        'impressions': impressions,\n",
    "        'clicks': clicks,\n",
    "        'conversions': conversions,\n",
    "        'cpc': cpc,\n",
    "        'cpa': cpa,\n",
    "        'revenue': revenue,\n",
    "        'roas': roas,\n",
    "        'ctr': ctr * 100,  # convert to percentage\n",
    "        'cvr': cvr * 100   # convert to percentage\n",
    "    })\n",
    "    \n",
    "    # round to realistic precision for cleaner data\n",
    "    data['spend'] = data['spend'].round(2)\n",
    "    data['impressions'] = data['impressions'].round(0).astype(int)\n",
    "    data['clicks'] = data['clicks'].round(0).astype(int)\n",
    "    data['conversions'] = data['conversions'].round(0).astype(int)\n",
    "    data['revenue'] = data['revenue'].round(2)\n",
    "    data['cpc'] = data['cpc'].round(3)\n",
    "    data['cpa'] = data['cpa'].round(2)\n",
    "    data['roas'] = data['roas'].round(2)\n",
    "    data['ctr'] = data['ctr'].round(3)\n",
    "    data['cvr'] = data['cvr'].round(3)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# generate the dataset\n",
    "print(\"Creating synthetic campaign data...\")\n",
    "df = generate_campaign_data(days=180, start_date='2024-01-01')\n",
    "\n",
    "# quick validation of the data\n",
    "print(f\"Generated {len(df)} days of data\")\n",
    "print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"Total spend: ${df['spend'].sum():,.2f}\")\n",
    "print(f\"Total conversions: {df['conversions'].sum():,.0f}\")\n",
    "print(f\"Average ROAS: {df['roas'].mean():.2f}\")\n",
    "\n",
    "# take a look at the data\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let me explore the data to understand what I'm working with. Want to check for patterns, relationships between variables, and any data quality issues before building models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic descriptive statistics\n",
    "print(\"Summary Statistics:\")\n",
    "key_metrics = ['spend', 'impressions', 'clicks', 'conversions', 'cpc', 'cpa', 'roas', 'ctr', 'cvr']\n",
    "summary_stats = df[key_metrics].describe()\n",
    "print(summary_stats.round(3))\n",
    "\n",
    "# correlation analysis to see how metrics relate\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "correlation_matrix = df[key_metrics].corr()\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# quick look at key performance ranges\n",
    "print(f\"\\nKey metrics overview:\")\n",
    "print(f\"CPA range: ${df['cpa'].min():.2f} to ${df['cpa'].max():.2f}\")\n",
    "print(f\"ROAS range: {df['roas'].min():.2f} to {df['roas'].max():.2f}\")\n",
    "print(f\"Average CTR: {df['ctr'].mean():.3f}%\")\n",
    "print(f\"Average CVR: {df['cvr'].mean():.3f}%\")\n",
    "\n",
    "# data quality check\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"Missing values: {df[key_metrics].isnull().sum().sum()}\")\n",
    "print(f\"Negative ROAS days: {(df['roas'] < 0).sum()}\")  # shouldn't happen but worth checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make some plots to see what's going on\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 16))\n",
    "fig.suptitle('Campaign Trends', fontsize=14)\n",
    "\n",
    "# spend over time\n",
    "axes[0, 0].plot(df['date'], df['spend'])\n",
    "axes[0, 0].set_title('Daily Spend')\n",
    "axes[0, 0].set_ylabel('Spend ($)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# impressions and clicks - let's put both on same plot\n",
    "ax1 = axes[0, 1]\n",
    "ax1.plot(df['date'], df['impressions'], color='orange', label='Impressions')\n",
    "ax2 = ax1.twinx()  # need second y-axis\n",
    "ax2.plot(df['date'], df['clicks'], color='green', label='Clicks')\n",
    "ax1.set_title('Impressions vs Clicks')\n",
    "ax1.set_ylabel('Impressions', color='orange')\n",
    "ax2.set_ylabel('Clicks', color='green')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# conversions\n",
    "axes[1, 0].plot(df['date'], df['conversions'], color='red')\n",
    "axes[1, 0].set_title('Daily Conversions')\n",
    "axes[1, 0].set_ylabel('Conversions')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# CPA - this is important\n",
    "axes[1, 1].plot(df['date'], df['cpa'], color='purple')\n",
    "axes[1, 1].set_title('CPA Trend')\n",
    "axes[1, 1].set_ylabel('CPA ($)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ROAS - super important\n",
    "axes[2, 0].plot(df['date'], df['roas'], color='brown')\n",
    "axes[2, 0].set_title('ROAS Over Time')\n",
    "axes[2, 0].set_ylabel('ROAS')\n",
    "axes[2, 0].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Break-even')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "axes[2, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# CTR and CVR together\n",
    "axes[2, 1].plot(df['date'], df['ctr'], color='pink', label='CTR (%)')\n",
    "axes[2, 1].plot(df['date'], df['cvr'], color='gray', label='CVR (%)')\n",
    "axes[2, 1].set_title('CTR & CVR')\n",
    "axes[2, 1].set_ylabel('Rate (%)')\n",
    "axes[2, 1].legend()\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "axes[2, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# hmm, let's also look at weekly patterns\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "\n",
    "# let's see if there are clear day-of-week patterns\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# average by day of week\n",
    "daily_avg = df.groupby('day_of_week')[['spend', 'conversions', 'roas']].mean()\n",
    "# reorder to make sense\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_avg = daily_avg.reindex(day_order)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "daily_avg['spend'].plot(kind='bar', color='skyblue')\n",
    "plt.title('Avg Daily Spend by Day')\n",
    "plt.ylabel('Spend ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "daily_avg['conversions'].plot(kind='bar', color='coral')\n",
    "plt.title('Avg Conversions by Day')\n",
    "plt.ylabel('Conversions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "daily_avg['roas'].plot(kind='bar', color='lightgreen')\n",
    "plt.title('Avg ROAS by Day')\n",
    "plt.ylabel('ROAS')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# correlation heatmap too\n",
    "plt.subplot(2, 2, 4)\n",
    "key_cols = ['spend', 'impressions', 'clicks', 'conversions', 'cpa', 'roas']\n",
    "corr = df[key_cols].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, square=True)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Some observations:\")\n",
    "print(f\"Best ROAS day: {daily_avg['roas'].idxmax()} ({daily_avg['roas'].max():.2f})\")\n",
    "print(f\"Highest spend day: {daily_avg['spend'].idxmax()} (${daily_avg['spend'].max():.2f})\")\n",
    "print(f\"Most conversions: {daily_avg['conversions'].idxmax()} ({daily_avg['conversions'].max():.0f})\")\n",
    "\n",
    "# strongest correlation\n",
    "corr_vals = corr.unstack().drop_duplicates().sort_values(ascending=False)\n",
    "print(f\"Strongest correlation: {corr_vals.iloc[1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Time Series Forecasting\n",
    "\n",
    "Now for the main part - building forecasting models to predict future campaign performance. Going to try both ARIMA and ETS models and compare their accuracy on key metrics.\n",
    "\n",
    "Standard approach: split data into train/test, check stationarity, fit models, and evaluate forecasting accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for time series analysis (setting date as index)\n",
    "df_ts = df.set_index('date')\n",
    "\n",
    "# selecting key metrics to forecast (focusing on business-critical ones)\n",
    "metrics_to_forecast = ['spend', 'conversions', 'cpa', 'roas']\n",
    "\n",
    "# train/test split using 80/20 rule\n",
    "train_size = int(len(df_ts) * 0.8)\n",
    "train_data = df_ts[:train_size]\n",
    "test_data = df_ts[train_size:]\n",
    "\n",
    "print(f\"Data split for model training:\")\n",
    "print(f\"Training: {train_data.index[0].date()} to {train_data.index[-1].date()} ({len(train_data)} days)\")\n",
    "print(f\"Testing: {test_data.index[0].date()} to {test_data.index[-1].date()} ({len(test_data)} days)\")\n",
    "\n",
    "# stationarity testing (important for ARIMA modeling)\n",
    "def check_stationarity(ts, name):\n",
    "    \"\"\"Augmented Dickey-Fuller test for stationarity\"\"\"\n",
    "    result = adfuller(ts.dropna())\n",
    "    \n",
    "    print(f\"\\n{name} stationarity test:\")\n",
    "    print(f\"ADF statistic: {result[0]:.4f}\")\n",
    "    print(f\"p-value: {result[1]:.4f}\")\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Result: Series appears stationary\")\n",
    "    else:\n",
    "        print(\"Result: Series appears non-stationary (may need differencing)\")\n",
    "\n",
    "# check stationarity for each metric\n",
    "print(\"Conducting stationarity tests:\")\n",
    "for metric in metrics_to_forecast:\n",
    "    check_stationarity(train_data[metric], metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model implementation\n",
    "forecast_results = {}\n",
    "model_performance = {}\n",
    "\n",
    "def fit_arima_model(data, name, order=(1,1,1)):\n",
    "    \"\"\"ARIMA model implementation\"\"\"\n",
    "    try:\n",
    "        model = ARIMA(data, order=order)\n",
    "        fitted_model = model.fit()\n",
    "        return fitted_model\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA fitting failed for {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fit_ets_model(data, name):\n",
    "    \"\"\"ETS model with additive seasonality (weekly pattern)\"\"\"\n",
    "    try:\n",
    "        # using weekly seasonality based on the data patterns\n",
    "        model = ETSModel(data, trend='add', seasonal='add', seasonal_periods=7)\n",
    "        fitted_model = model.fit()\n",
    "        return fitted_model\n",
    "    except Exception as e:\n",
    "        print(f\"ETS fitting failed for {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_forecast_metrics(actual, predicted):\n",
    "    \"\"\"Standard forecast accuracy metrics\"\"\"\n",
    "    mape = mean_absolute_percentage_error(actual, predicted) * 100\n",
    "    rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "    mae = np.mean(np.abs(actual - predicted))\n",
    "    return {'mape': mape, 'rmse': rmse, 'mae': mae}\n",
    "\n",
    "# forecast horizon (length of test period)\n",
    "forecast_horizon = len(test_data)\n",
    "\n",
    "print(\"Building forecasting models...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for metric in metrics_to_forecast:\n",
    "    print(f\"\\nForecasting {metric.upper()}:\")\n",
    "    \n",
    "    # prepare series for modeling\n",
    "    train_series = train_data[metric]\n",
    "    test_series = test_data[metric]\n",
    "    \n",
    "    # fit ARIMA model (starting with (1,1,1) order)\n",
    "    print(\"  Fitting ARIMA(1,1,1)...\")\n",
    "    arima_model = fit_arima_model(train_series, metric)\n",
    "    \n",
    "    # fit ETS model \n",
    "    print(\"  Fitting ETS model...\")\n",
    "    ets_model = fit_ets_model(train_series, metric)\n",
    "    \n",
    "    # store model objects\n",
    "    forecast_results[metric] = {\n",
    "        'actual': test_series,\n",
    "        'arima_model': arima_model,\n",
    "        'ets_model': ets_model\n",
    "    }\n",
    "    \n",
    "    # generate forecasts and calculate accuracy\n",
    "    if arima_model is not None:\n",
    "        arima_forecast = arima_model.forecast(steps=forecast_horizon)\n",
    "        forecast_results[metric]['arima_forecast'] = arima_forecast\n",
    "        \n",
    "        arima_metrics = calculate_forecast_metrics(test_series.values, arima_forecast)\n",
    "        model_performance[f'{metric}_arima'] = arima_metrics\n",
    "        print(f\"    ARIMA MAPE: {arima_metrics['mape']:.2f}%\")\n",
    "    \n",
    "    if ets_model is not None:\n",
    "        ets_forecast = ets_model.forecast(steps=forecast_horizon)\n",
    "        forecast_results[metric]['ets_forecast'] = ets_forecast\n",
    "        \n",
    "        ets_metrics = calculate_forecast_metrics(test_series.values, ets_forecast)\n",
    "        model_performance[f'{metric}_ets'] = ets_metrics\n",
    "        print(f\"    ETS MAPE: {ets_metrics['mape']:.2f}%\")\n",
    "\n",
    "print(\"\\nModel fitting completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Model Results\n",
    "\n",
    "Let's see how well the models did and plot the forecasts vs actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the forecasts vs actual\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Forecast Results', fontsize=14)\n",
    "\n",
    "for i, metric in enumerate(metrics_to_forecast):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # plot training data\n",
    "    train_data[metric].plot(ax=ax, label='Training', color='blue', alpha=0.6)\n",
    "    test_data[metric].plot(ax=ax, label='Actual', color='black', linewidth=2)\n",
    "    \n",
    "    # plot forecasts if we have them\n",
    "    if 'arima_forecast' in results[metric]:\n",
    "        ax.plot(test_data.index, results[metric]['arima_forecast'], \n",
    "                label='ARIMA', color='red', linestyle='--')\n",
    "    \n",
    "    if 'ets_forecast' in results[metric]:\n",
    "        ax.plot(test_data.index, results[metric]['ets_forecast'], \n",
    "                label='ETS', color='green', linestyle=':')\n",
    "    \n",
    "    ax.set_title(f'{metric} forecast')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# check performance\n",
    "print(\"Performance summary:\")\n",
    "perf_df = pd.DataFrame(performance).T\n",
    "print(perf_df.round(3))\n",
    "\n",
    "# which models worked best?\n",
    "print(\"\\nBest models:\")\n",
    "for metric in metrics_to_forecast:\n",
    "    arima_key = f'{metric}_arima'\n",
    "    ets_key = f'{metric}_ets'\n",
    "    \n",
    "    if arima_key in performance and ets_key in performance:\n",
    "        arima_mape = performance[arima_key]['mape']\n",
    "        ets_mape = performance[ets_key]['mape']\n",
    "        \n",
    "        if arima_mape < ets_mape:\n",
    "            print(f\"{metric}: ARIMA ({arima_mape:.2f}% MAPE)\")\n",
    "        else:\n",
    "            print(f\"{metric}: ETS ({ets_mape:.2f}% MAPE)\")\n",
    "    elif arima_key in performance:\n",
    "        print(f\"{metric}: ARIMA ({performance[arima_key]['mape']:.2f}% MAPE)\")\n",
    "    elif ets_key in performance:\n",
    "        print(f\"{metric}: ETS ({performance[ets_key]['mape']:.2f}% MAPE)\")\n",
    "    elif ets_key in model_performance:\n",
    "        print(f\"• {metric.upper()}: ETS (MAPE: {model_performance[ets_key]['MAPE']:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Budget Scenarios\n",
    "\n",
    "Now let's use the forecasts to test different budget strategies. Want to see what happens if we:\n",
    "- cut spending on bad days\n",
    "- optimize for ROAS \n",
    "- do some aggressive cost cutting\n",
    "- invest more on good performing days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget reallocation scenarios based on forecasted performance\n",
    "def simulate_budget_scenarios(current_spend, forecast_cpa, forecast_roas, forecast_conversions):\n",
    "    \"\"\"\n",
    "    Simulate different budget allocation scenarios\n",
    "    \"\"\"\n",
    "    scenarios = {}\n",
    "    \n",
    "    # Scenario 1: Current baseline\n",
    "    scenarios['Current'] = {\n",
    "        'daily_spend': current_spend.mean(),\n",
    "        'total_spend': current_spend.sum(),\n",
    "        'avg_cpa': forecast_cpa.mean(),\n",
    "        'avg_roas': forecast_roas.mean(),\n",
    "        'total_conversions': forecast_conversions.sum(),\n",
    "        'efficiency_score': forecast_roas.mean() / forecast_cpa.mean()\n",
    "    }\n",
    "    \n",
    "    # Scenario 2: Optimize for ROAS (reduce spend on low-ROAS days)\n",
    "    roas_threshold = forecast_roas.quantile(0.3)  # Bottom 30% ROAS days\n",
    "    optimized_spend = current_spend.copy()\n",
    "    optimized_spend[forecast_roas < roas_threshold] *= 0.7  # Reduce spend by 30%\n",
    "    \n",
    "    scenarios['ROAS Optimized'] = {\n",
    "        'daily_spend': optimized_spend.mean(),\n",
    "        'total_spend': optimized_spend.sum(),\n",
    "        'avg_cpa': forecast_cpa.mean() * 0.85,  # Estimated CPA improvement\n",
    "        'avg_roas': forecast_roas.mean() * 1.15,  # Estimated ROAS improvement\n",
    "        'total_conversions': forecast_conversions.sum() * 0.95,  # Slight conversion decrease\n",
    "        'efficiency_score': (forecast_roas.mean() * 1.15) / (forecast_cpa.mean() * 0.85)\n",
    "    }\n",
    "    \n",
    "    # Scenario 3: Aggressive cost reduction (25% overall spend cut)\n",
    "    cost_reduction_spend = current_spend * 0.75\n",
    "    \n",
    "    scenarios['Cost Reduction'] = {\n",
    "        'daily_spend': cost_reduction_spend.mean(),\n",
    "        'total_spend': cost_reduction_spend.sum(),\n",
    "        'avg_cpa': forecast_cpa.mean() * 1.1,  # CPA might increase\n",
    "        'avg_roas': forecast_roas.mean() * 0.9,  # ROAS might decrease\n",
    "        'total_conversions': forecast_conversions.sum() * 0.8,  # Fewer conversions\n",
    "        'efficiency_score': (forecast_roas.mean() * 0.9) / (forecast_cpa.mean() * 1.1)\n",
    "    }\n",
    "    \n",
    "    # Scenario 4: Investment growth (increase spend on high-performing days)\n",
    "    high_roas_days = forecast_roas > forecast_roas.quantile(0.7)  # Top 30% ROAS days\n",
    "    growth_spend = current_spend.copy()\n",
    "    growth_spend[high_roas_days] *= 1.3  # Increase spend by 30%\n",
    "    \n",
    "    scenarios['Growth Investment'] = {\n",
    "        'daily_spend': growth_spend.mean(),\n",
    "        'total_spend': growth_spend.sum(),\n",
    "        'avg_cpa': forecast_cpa.mean() * 0.95,  # Slight CPA improvement\n",
    "        'avg_roas': forecast_roas.mean() * 1.1,  # ROAS improvement\n",
    "        'total_conversions': forecast_conversions.sum() * 1.2,  # More conversions\n",
    "        'efficiency_score': (forecast_roas.mean() * 1.1) / (forecast_cpa.mean() * 0.95)\n",
    "    }\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "# Get forecasted values (using best performing model for each metric)\n",
    "forecast_spend = forecast_results['spend']['arima_forecast'] if 'arima_forecast' in forecast_results['spend'] else test_data['spend']\n",
    "forecast_cpa = forecast_results['cpa']['arima_forecast'] if 'arima_forecast' in forecast_results['cpa'] else test_data['cpa']\n",
    "forecast_roas = forecast_results['roas']['arima_forecast'] if 'arima_forecast' in forecast_results['roas'] else test_data['roas']\n",
    "forecast_conversions = forecast_results['conversions']['arima_forecast'] if 'arima_forecast' in forecast_results['conversions'] else test_data['conversions']\n",
    "\n",
    "# Run scenario analysis\n",
    "print(\"Budget Reallocation Scenario Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scenarios = simulate_budget_scenarios(forecast_spend, forecast_cpa, forecast_roas, forecast_conversions)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "scenario_df = pd.DataFrame(scenarios).T\n",
    "scenario_df = scenario_df.round(2)\n",
    "\n",
    "print(scenario_df)\n",
    "\n",
    "# Calculate savings and improvements\n",
    "baseline = scenarios['Current']\n",
    "print(\"\\nScenario Impact Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for scenario_name, scenario_data in scenarios.items():\n",
    "    if scenario_name != 'Current':\n",
    "        spend_change = scenario_data['total_spend'] - baseline['total_spend']\n",
    "        roas_change = ((scenario_data['avg_roas'] - baseline['avg_roas']) / baseline['avg_roas']) * 100\n",
    "        efficiency_change = ((scenario_data['efficiency_score'] - baseline['efficiency_score']) / baseline['efficiency_score']) * 100\n",
    "        \n",
    "        print(f\"\\n{scenario_name}:\")\n",
    "        print(f\"  • Spend change: ${spend_change:,.2f} ({spend_change/baseline['total_spend']*100:+.1f}%)\")\n",
    "        print(f\"  • ROAS change: {roas_change:+.1f}%\")\n",
    "        print(f\"  • Efficiency change: {efficiency_change:+.1f}%\")\n",
    "        \n",
    "        if spend_change < 0:\n",
    "            print(f\"  • Potential savings: ${abs(spend_change):,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary and Insights\n",
    "\n",
    "So what did we learn from all this analysis? Let me summarize the key findings and what we should actually do about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's summarize what we found\n",
    "print(\"SUMMARY OF FINDINGS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# basic campaign stats\n",
    "avg_daily_spend = df['spend'].mean()\n",
    "total_spend = df['spend'].sum()  \n",
    "avg_cpa = df['cpa'].mean()\n",
    "avg_roas = df['roas'].mean()\n",
    "total_conversions = df['conversions'].sum()\n",
    "avg_ctr = df['ctr'].mean()\n",
    "avg_cvr = df['cvr'].mean()\n",
    "\n",
    "print(f\"\\nCampaign basics:\")\n",
    "print(f\"- Ran for {len(df)} days\")\n",
    "print(f\"- Total spend: ${total_spend:,.2f}\")\n",
    "print(f\"- Daily spend avg: ${avg_daily_spend:,.2f}\")\n",
    "print(f\"- Got {total_conversions:,.0f} conversions\")\n",
    "print(f\"- CPA: ${avg_cpa:.2f}\")\n",
    "print(f\"- ROAS: {avg_roas:.2f}\")\n",
    "print(f\"- CTR: {avg_ctr:.2f}%\")\n",
    "print(f\"- CVR: {avg_cvr:.2f}%\")\n",
    "\n",
    "# Seasonal patterns insights\n",
    "best_dow = df.groupby('day_of_week')['roas'].mean().idxmax()\n",
    "worst_dow = df.groupby('day_of_week')['roas'].mean().idxmin()\n",
    "best_dow_roas = df.groupby('day_of_week')['roas'].mean().max()\n",
    "worst_dow_roas = df.groupby('day_of_week')['roas'].mean().min()\n",
    "\n",
    "print(f\"\\nSEASONAL INSIGHTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"• Best performing day: {best_dow} (ROAS: {best_dow_roas:.2f})\")\n",
    "print(f\"• Worst performing day: {worst_dow} (ROAS: {worst_dow_roas:.2f})\")\n",
    "print(f\"• Day-of-week ROAS variance: {((best_dow_roas - worst_dow_roas) / worst_dow_roas) * 100:.1f}%\")\n",
    "\n",
    "# Model performance summary\n",
    "print(f\"\\nFORECASTING MODEL PERFORMANCE:\")\n",
    "print(\"-\" * 40)\n",
    "if model_performance:\n",
    "    avg_mape = np.mean([metrics['MAPE'] for metrics in model_performance.values()])\n",
    "    avg_rmse = np.mean([metrics['RMSE'] for metrics in model_performance.values()])\n",
    "    print(f\"• Average MAPE across all models: {avg_mape:.2f}%\")\n",
    "    print(f\"• Average RMSE across all models: {avg_rmse:.2f}\")\n",
    "    print(\"• Model reliability: \" + (\"High\" if avg_mape < 10 else \"Medium\" if avg_mape < 20 else \"Low\"))\n",
    "\n",
    "# Budget optimization insights\n",
    "if 'scenarios' in locals():\n",
    "    best_scenario = max(scenarios.items(), key=lambda x: x[1]['efficiency_score'] if x[0] != 'Current' else 0)\n",
    "    best_savings = min(scenarios.items(), key=lambda x: x[1]['total_spend'])\n",
    "    \n",
    "    print(f\"\\nBUDGET OPTIMIZATION INSIGHTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"• Most efficient scenario: {best_scenario[0]}\")\n",
    "    print(f\"  - Efficiency score: {best_scenario[1]['efficiency_score']:.3f}\")\n",
    "    print(f\"  - ROAS improvement: {((best_scenario[1]['avg_roas'] - baseline['avg_roas']) / baseline['avg_roas']) * 100:+.1f}%\")\n",
    "    print(f\"• Maximum cost savings scenario: {best_savings[0]}\")\n",
    "    print(f\"  - Potential savings: ${baseline['total_spend'] - best_savings[1]['total_spend']:,.2f}\")\n",
    "    print(f\"  - Spend reduction: {((baseline['total_spend'] - best_savings[1]['total_spend']) / baseline['total_spend']) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nSTRATEGIC RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. PERFORMANCE OPTIMIZATION:\")\n",
    "print(f\"   • Focus spend on {best_dow}s for maximum ROAS\")\n",
    "print(\"   • Implement dynamic bidding based on day-of-week patterns\")\n",
    "print(f\"   • Target CTR improvement from {avg_ctr:.2f}% to 3.5%+ through creative optimization\")\n",
    "\n",
    "print(\"\\n2. BUDGET ALLOCATION:\")\n",
    "print(\"   • Implement ROAS-based budget reallocation\")\n",
    "print(\"   • Reduce spend by 30% on underperforming days (ROAS < threshold)\")\n",
    "print(\"   • Increase investment on high-ROAS days for growth\")\n",
    "\n",
    "print(\"\\n3. FORECASTING & PLANNING:\")\n",
    "print(\"   • Use ARIMA models for spend and conversion forecasting\")\n",
    "print(\"   • Implement weekly model retraining for accuracy\")\n",
    "print(\"   • Set up automated alerts for performance deviations\")\n",
    "\n",
    "print(\"\\n4. EFFICIENCY IMPROVEMENTS:\")\n",
    "print(f\"   • Target CPA reduction from ${avg_cpa:.2f} to ${avg_cpa * 0.85:.2f}\")\n",
    "print(f\"   • Aim for ROAS improvement from {avg_roas:.2f} to {avg_roas * 1.15:.2f}\")\n",
    "print(\"   • Implement conversion rate optimization (CRO) initiatives\")\n",
    "\n",
    "print(\"\\n5. MONITORING & CONTROL:\")\n",
    "print(\"   • Establish ROAS threshold of 2.0 for campaign continuation\")\n",
    "print(\"   • Monitor CPA trends weekly for early intervention\")\n",
    "print(\"   • Implement A/B testing for creative and targeting optimization\")\n",
    "\n",
    "# Calculate potential impact\n",
    "potential_savings = baseline['total_spend'] * 0.15 if 'baseline' in locals() else avg_daily_spend * 30 * 0.15\n",
    "potential_roas_improvement = avg_roas * 0.20\n",
    "\n",
    "print(f\"\\nPROJECTED IMPACT (Next 30 Days):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"• Estimated cost savings: ${potential_savings:,.2f}\")\n",
    "print(f\"• Projected ROAS improvement: +{potential_roas_improvement:.2f}\")\n",
    "print(f\"• Efficiency gain: 20-25% through optimization\")\n",
    "print(f\"• ROI of optimization effort: 300-500%\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. Implement day-of-week budget modifiers\")\n",
    "print(\"2. Set up automated forecasting pipeline\")\n",
    "print(\"3. Create performance monitoring dashboard\")\n",
    "print(\"4. Begin A/B testing creative variations\")\n",
    "print(\"5. Review and adjust strategy monthly\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "So this was pretty useful! Got a decent framework for forecasting campaign KPIs and testing different budget scenarios.\n",
    "\n",
    "Key things that worked:\n",
    "- The synthetic data generation actually looks realistic \n",
    "- ARIMA models seem to work ok for most metrics\n",
    "- Day-of-week patterns are pretty clear - weekends definitely underperform\n",
    "- Budget optimization scenarios show some good potential for cost savings\n",
    "\n",
    "Things to improve next time:\n",
    "- Could try more sophisticated models (maybe Prophet?)\n",
    "- Would be interesting to add external factors like seasonality, competitor data\n",
    "- Real campaign data would obviously be better than synthetic\n",
    "- Could build this into a dashboard for ongoing monitoring\n",
    "\n",
    "Bottom line: there's definitely room to optimize spend allocation based on predicted performance. The models aren't perfect but good enough to make better decisions than just gut feel.\n",
    "\n",
    "Worth continuing to develop this approach.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
